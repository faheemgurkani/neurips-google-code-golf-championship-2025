# ğŸ§© NeurIPS Google Code Golf Championship 2025 - Self-Verifying Generation Pipeline

An automated pipeline with **iterative refinement** for generating and validating code solutions for the NeurIPS 2025 Google Code Golf Championship using LLM (Groq).

## ğŸ¯ Overview

This pipeline automates the entire process of:

1. Loading task data (train/test examples from JSON files)
2. Generating Python solutions using LLM (Groq API with reasoning models)
3. **Iterative refinement**: Learning from previous failed attempts
4. Validating code syntax and correctness
5. Saving only verified solutions

## ğŸ“‹ Features

- âœ… **Iterative Refinement**: Model learns from previous failed attempts
- âœ… **Reasoning Model Support**: Uses GPT-4 OSS reasoning model for complex pattern discovery
- âœ… **Automated Validation**: Every generated solution is automatically tested
- âœ… **Syntax Checking**: Catches invalid Python code before validation
- âœ… **Functional Testing**: Tests solutions on training examples using `code_golf_utils`
- âœ… **Failed Attempt Tracking**: Saves failed code for debugging and analysis
- âœ… **Progress Tracking**: CSV logs with detailed statistics
- âœ… **Smart Retry Logic**: Passes previous errors to model for targeted fixes
- âœ… **Rate Limiting**: Built-in delays to respect API limits
- âœ… **Modular Design**: Easy to extend and customize

## ğŸ“ Directory Structure

```
neurips-google-code-golf-championship-2025/
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ modules/              # Core pipeline modules
â”‚   â”‚   â”œâ”€â”€ task_manager.py   # Manages task file loading
â”‚   â”‚   â”œâ”€â”€ task_parser.py    # Parses JSON task data
â”‚   â”‚   â”œâ”€â”€ prompt_builder.py # Builds LLM prompts (with iterative refinement)
â”‚   â”‚   â”œâ”€â”€ llm_generator.py  # Groq API integration
â”‚   â”‚   â”œâ”€â”€ output_parser.py  # Extracts code from LLM output
â”‚   â”‚   â”œâ”€â”€ code_validator.py # Validates syntax & correctness
â”‚   â”‚   â”œâ”€â”€ file_writer.py    # Saves validated solutions
â”‚   â”‚   â””â”€â”€ progress_tracker.py # Logs progress to CSV
â”‚   â”œâ”€â”€ tests/                # Test suite
â”‚   â”‚   â””â”€â”€ test_pipeline.py  # Component tests
â”‚   â”œâ”€â”€ output/               # Generated solutions (.py files)
â”‚   â”‚   â””â”€â”€ failed/           # Failed attempts for debugging
â”‚   â””â”€â”€ logs/                 # Progress logs (CSV)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ google-code-golf-2025/ # Task data files (task001.json - task400.json)
â”œâ”€â”€ run_pipeline.py           # Main orchestrator (with iterative refinement)
â”œâ”€â”€ setup.sh                  # Setup script
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ .env.template             # Environment template
â”œâ”€â”€ .gitignore                # Git ignore rules
â””â”€â”€ README.md                 # This file
```

## ğŸš€ Quick Start

### Step 1: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 2: Setup Environment

```bash
# Copy the template
cp .env.template .env

# Edit .env and add your Groq API key
# Get one from: https://console.groq.com/
```

Edit `.env`:

```env
GROQ_API_KEY=gsk_your_actual_api_key_here
```

### Step 3: Create Output Directories

```bash
# Run setup script
bash setup.sh
```

Or manually:

```bash
mkdir -p pipeline/output pipeline/output/failed pipeline/logs
```

### Step 4: Test the Pipeline

```bash
# Run tests to verify everything works
python pipeline/tests/test_pipeline.py
```

Expected output:

```
âœ“ All modules imported successfully
âœ“ TaskManager found 5 task files
âœ“ TaskParser loaded task with examples
âœ“ PromptBuilder created prompt
âœ“ CodeValidator working correctly
âœ“ OutputParser extracted code
```

### Step 5: Run on a Single Task (Testing)

```bash
# Test on task 1 only (recommended first run)
python run_pipeline.py --start 1 --end 1 --max-calls 5
```

This will:

- Generate solutions for task 1
- Automatically retry up to 3 times with **iterative refinement**
- Each retry learns from the previous failed attempt
- Validate each solution
- Save validated solution to `pipeline/output/`
- Save failed attempts to `pipeline/output/failed/` for debugging
- Log progress to `pipeline/logs/progress.csv`

### Step 6: Run on a Small Subset

```bash
# Process tasks 1-10
python run_pipeline.py --start 1 --end 10 --max-calls 50
```

### Step 7: Process All Tasks

```bash
# Process all tasks 1-400 (adjust max-calls based on your API limit)
python run_pipeline.py --start 1 --end 400 --max-calls 1200
```

## ğŸ“Š Output

### Generated Solutions

Each validated solution is saved as:

```
pipeline/output/task001.py
pipeline/output/task002.py
...
```

### Failed Attempts (for Debugging)

Failed attempts are saved for analysis:

```
pipeline/output/failed/task001_attempt1.py
pipeline/output/failed/task001_attempt2.py
...
```

You can compare attempts to see how the model evolved:

```bash
# See what changed between attempts
diff pipeline/output/failed/task001_attempt1.py pipeline/output/failed/task001_attempt2.py
```

### Progress Logs

CSV logs are saved to `pipeline/logs/progress.csv` with columns:

- `timestamp`: When the event occurred
- `task_id`: Task identifier
- `status`: Status (validated_success, syntax_error, failed_validation, etc.)
- `code_length`: Size of generated code in bytes
- `validation_passed`: Boolean
- `details`: Additional information

### Monitor Progress

```bash
# Watch the log file in real-time
tail -f pipeline/logs/progress.csv

# Check the output directory
ls -lh pipeline/output/

# Count successes
grep "validated_success" pipeline/logs/progress.csv | wc -l
```

## âš™ï¸ Configuration

### Environment Variables (.env)

```env
GROQ_API_KEY=your_groq_api_key_here
```

### Command Line Arguments

```bash
python run_pipeline.py \
  --start 1 \
  --end 100 \
  --max-calls 300 \
  --model openai/gpt-oss-20b
```

| Option        | Default              | Description                         |
| ------------- | -------------------- | ----------------------------------- |
| `--start`     | `1`                  | Starting task number                |
| `--end`       | `100`                | Ending task number                  |
| `--max-calls` | `100`                | Max API calls (tasks Ã— max_retries) |
| `--model`     | `openai/gpt-oss-20b` | Groq model to use                   |

**Note**: `--max-calls` limits the total number of API calls, not tasks. With 3 attempts per task, processing 100 tasks could use up to 300 API calls.

### Available Models

**Recommended (Reasoning Models)**:

- `openai/gpt-oss-20b` âœ… **(default)** - GPT-4 OSS reasoning model, excellent for complex patterns
- `llama-3.3-70b-versatile` - High quality, versatile model

**Fast Models**:

- `llama-3.1-8b-instant` - Fastest, good for simple tasks
- `mixtral-8x7b-32768` - Balanced speed and quality

## ğŸ” How It Works

### Pipeline Stages with Iterative Refinement

```
1. Task Manager â†’ Finds and loads task files
2. Task Parser â†’ Extracts train/test examples
3. Prompt Builder â†’ Creates LLM prompt with examples
   â”œâ”€ Attempt 1: Generic pattern discovery prompt
   â”œâ”€ Attempt 2: Includes previous failed code + error
   â””â”€ Attempt 3: Includes previous failed code + error
4. LLM Generator â†’ Calls Groq API to generate code
5. Output Parser â†’ Extracts Python code from response
6. Code Validator â†’ Checks syntax and tests on training data
   â”œâ”€ If PASS: Continue to step 7
   â””â”€ If FAIL: Store error, go back to step 3 (next attempt)
7. File Writer â†’ Saves validated solutions
8. Progress Tracker â†’ Logs all results
```

### Iterative Refinement Process

For each task, the pipeline tries up to 3 times:

**Attempt 1 (Initial)**:

- Uses a generic prompt focused on pattern discovery
- Analyzes dimensions, tiling patterns, transformations
- No prior context

**Attempt 2 (Refinement)**:

```
Previous failed code â†’ Analyze what went wrong â†’ Generate improved solution
```

- Receives the failed code from Attempt 1
- Gets the specific error message
- Model analyzes the failure and corrects its approach

**Attempt 3 (Final Refinement)**:

```
Previous failed code â†’ Analyze what went wrong â†’ Generate improved solution
```

- Receives the failed code from Attempt 2
- Gets the specific error message
- Final attempt with all learned context

### Validation Process

For each generated solution:

1. **Syntax Check**

   ```python
   ast.parse(code)  # Must be valid Python
   ```

2. **Module Import**

   ```python
   module = importlib.util.module_from_spec(spec)
   spec.loader.exec_module(module)
   program = getattr(module, 'p')
   ```

3. **Functional Test**

   ```python
   for example in train_examples:
       result = program(copy.deepcopy(example["input"]))
       assert np.array_equal(result, example["output"])
   ```

4. **Save if Valid**
   - Only saves if 100% pass rate on training examples
   - Logs all results to CSV

## ğŸ› ï¸ Module Details

### TaskManager (`modules/task_manager.py`)

- Manages task file paths (task001-task400)
- Handles task numbering
- Validates file existence

### TaskParser (`modules/task_parser.py`)

- Loads JSON task files
- Extracts train/test examples
- Formats examples for prompts

### PromptBuilder (`modules/prompt_builder.py`)

- **Dual prompt system**: Initial + Refinement templates
- **Initial prompt**: Generic pattern discovery with dimension analysis
- **Refinement prompt**: Includes previous failed code and error message
- Optimized for code golf constraints
- Includes train and test examples

### LLMCodeGenerator (`modules/llm_generator.py`)

- Groq API integration
- Default model: `openai/gpt-oss-20b` (GPT-4 OSS reasoning)
- Configurable temperature/model
- High token limit (8000) for reasoning space
- Built-in rate limiting
- Tracks API call count

### OutputParser (`modules/output_parser.py`)

- Extracts code from markdown blocks
- Handles various LLM output formats
- Cleans whitespace/formatting

### CodeValidator (`modules/code_validator.py`)

- Syntax validation using AST parsing
- Functional testing on training data
- Uses numpy for array comparison

### FileWriter (`modules/file_writer.py`)

- Saves validated solutions
- Organizes by task ID
- Tracks file existence

### ProgressTracker (`modules/progress_tracker.py`)

- CSV logging
- Statistics tracking
- Real-time progress monitoring

## ğŸ“ Example Usage

### Programmatic Usage with Iterative Refinement

```python
from pipeline.modules import (
    TaskManager, TaskParser, PromptBuilder,
    LLMCodeGenerator, CodeValidator, FileWriter,
    OutputParser
)

# Initialize components
script_dir = Path(__file__).parent
data_dir = script_dir / 'data' / 'google-code-golf-2025'
tm = TaskManager(data_dir, 1, 10)
tp = TaskParser()
pb = PromptBuilder()
llm = LLMCodeGenerator(model='openai/gpt-oss-20b')
cv = CodeValidator()
fw = FileWriter('pipeline/output')
op = OutputParser()

# Process a task with iterative refinement
task_path = data_dir / 'task001.json'
train, test = tp.load_task(task_path)
task_data = {"train": train, "test": test}

previous_code = None
previous_error = None
max_attempts = 3

for attempt in range(max_attempts):
    # Build prompt (with previous attempt info if available)
    if attempt == 0:
        prompt = pb.build_prompt(1, train, test)
    else:
        prompt = pb.build_prompt(
            1, train, test,
            previous_code=previous_code,
            error_message=previous_error
        )

    # Generate code
    raw_output = llm.generate(prompt)
    code = op.extract_code(raw_output)

    # Validate
    if cv.is_syntax_valid(code):
        passed, message = cv.passes_training_examples(task_data, code)
        if passed:
            fw.save_code(1, code)
            print(f"âœ… Success on attempt {attempt + 1}")
            break
        else:
            # Store for next attempt
            previous_code = code
            previous_error = message
    else:
        previous_code = code
        previous_error = "Syntax error"
```

## ğŸ“ˆ Statistics

The pipeline tracks:

- âœ… Successful validations
- âŒ Syntax errors
- âŒ Validation failures
- âš ï¸ Exceptions
- â­ï¸ Skipped (already solved)

## ğŸ“ Example Output

```
================================================================================
ğŸš€ Google Code Golf 2025 - Self-Verifying Generation Pipeline
================================================================================
ğŸ“ Data directory: /path/to/data/google-code-golf-2025
ğŸ’¾ Output directory: /path/to/pipeline/output
ğŸ“Š Tasks: 1 to 10
ğŸ¤– Model: openai/gpt-oss-20b
ğŸ”„ Max API calls: 100
================================================================================

ğŸ“‹ Found 10 task files

================================================================================
[1/10] Processing Task 001
================================================================================
ğŸ“š Train examples: 5
ğŸ§ª Test examples: 1
ğŸ¤– Generating code... (attempt 1/3) [Initial]
[LLM] API call #1 completed
ğŸ“ Generated code (0 bytes)
ğŸ” Validating syntax...
âœ… Validating functionality...
âŒ Validation failed: Function p() not found
ğŸ”„ Retrying with previous solution feedback...
ğŸ¤– Generating code... (attempt 2/3) [Refinement]
[LLM] API call #2 completed
ğŸ“ Generated code (305 bytes)
ğŸ” Validating syntax...
âœ… Validating functionality...
âœ… Validation passed! Passed 5/5 train examples
[âœ“] Saved validated solution for task 001 (305 bytes)

â±ï¸  Waiting 2s before next task...

================================================================================
[2/10] Processing Task 002
================================================================================
ğŸ“š Train examples: 5
ğŸ§ª Test examples: 1
ğŸ¤– Generating code... (attempt 1/3) [Initial]
[LLM] API call #3 completed
ğŸ“ Generated code (187 bytes)
ğŸ” Validating syntax...
âœ… Validating functionality...
âœ… Validation passed! Passed 5/5 train examples
[âœ“] Saved validated solution for task 002 (187 bytes)

...

================================================================================
ğŸ“Š PIPELINE SUMMARY
================================================================================
âœ… Successful: 8
âŒ Syntax errors: 0
âŒ Validation failed: 2
âŒ Exceptions: 0
â­ï¸  Skipped: 0
ğŸ“ API calls made: 28
================================================================================
```

**Key Features Shown**:

- `[Initial]` vs `[Refinement]` labels on attempts
- Shows when retrying with previous solution feedback
- Tracks API calls across all attempts
- Displays validation results for each attempt

## ğŸ“ Best Practices

1. **Start Small**: Test on task 1 first, then 1-10
2. **Monitor Logs**: Check `pipeline/logs/progress.csv` regularly
3. **Analyze Failures**: Review `pipeline/output/failed/` to understand why attempts failed
4. **Compare Attempts**: Use `diff` to see how the model evolved between attempts
5. **Adjust Temperature**: Lower (0.1) for consistency, higher (0.3) for creativity
6. **Use Reasoning Models**: `openai/gpt-oss-20b` works best for complex ARC patterns
7. **Check API Limits**: Monitor your Groq API usage (each task uses up to 3 calls)
8. **Budget API Calls**: Set `--max-calls` to `tasks Ã— 3` for full retry coverage

## ğŸ› Troubleshooting

### ModuleNotFoundError

```bash
# Make sure you're in the pipeline directory
cd pipeline

# And that Python can find modules
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

### API Key Not Found

```bash
# Check your .env file
cat .env | grep GROQ_API_KEY

# Verify key works
curl https://api.groq.com/v1/models -H "Authorization: Bearer $GROQ_API_KEY"
```

### Rate Limit Exceeded

- Wait a few minutes between runs
- Reduce `--max-calls` parameter
- Use a higher quality model but call less frequently

### Validation Failures

- Check task examples manually
- Try different models
- Adjust temperature (0.1-0.3)
- Enable retries in run_pipeline.py

## ğŸ§ª Running Tests

```bash
# Run all tests
python pipeline/tests/test_pipeline.py

# Test specific component
python -c "
import sys
from pathlib import Path
sys.path.insert(0, str(Path('pipeline') / 'modules'))
from pipeline.modules import TaskManager, TaskParser
tm = TaskManager('data/google-code-golf-2025', 1, 1)
paths = tm.get_task_paths()
tp = TaskParser()
data = tp.load_task(paths[0])
print(f'Task has {len(data[\"train\"])} train examples')
"
```

## ğŸ¯ Next Steps

1. **Test on one task**: `python run_pipeline.py --start 1 --end 1 --max-calls 5`
2. **Review the results**:
   - Successful solution: `pipeline/output/task001.py`
   - Failed attempts: `pipeline/output/failed/task001_attempt*.py`
   - Logs: `pipeline/logs/progress.csv`
3. **Analyze learning**: Compare failed attempts to see model evolution
4. **Run on subset**: `python run_pipeline.py --start 1 --end 10 --max-calls 50`
5. **Scale up**: `python run_pipeline.py --start 1 --end 400 --max-calls 1200`
6. **Submit validated solutions** to Kaggle

## ğŸ’¡ Understanding Iterative Refinement

### Example: Task 001 Evolution

**Attempt 1** (Initial - FAILED):

- Model generates basic approach
- Misunderstands the pattern
- Validation fails with specific error

**Attempt 2** (Refinement - SUCCESS):

```
Input: Previous failed code + Error message
Analysis: Model reviews what went wrong
Output: Corrected solution that passes validation âœ…
```

This approach significantly improves success rates on complex tasks!

### Debugging Failed Tasks

```bash
# See all attempts for a task
ls -lh pipeline/output/failed/task001_*

# Compare first and second attempts
diff pipeline/output/failed/task001_attempt1.py \
     pipeline/output/failed/task001_attempt2.py

# Count how many tasks failed all attempts
ls pipeline/output/failed/*attempt3.py | wc -l
```

## ğŸ“š Development

### Adding New Modules

1. Create module in `modules/`
2. Import in `modules/__init__.py`
3. Update `run_pipeline.py`
4. Test with `tests/test_pipeline.py`
5. Update documentation

### Debugging

Enable verbose output:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ“„ License

See the parent directory README for license information.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“§ Support

For issues or questions, please open a GitHub issue.

---

**Happy Golfing! â›³**
